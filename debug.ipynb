{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import onnxruntime as rt\n",
    "\n",
    "with open(\"input_t.pkl\", \"rb\") as f:\n",
    "    input_t = pickle.load(f)\n",
    "\n",
    "original_model_path = \"original_model.onnx\"\n",
    "session = rt.InferenceSession(model_path, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "\n",
    "def create_and_run_onnx_session(model_path: str, input_data: np.ndarray):\n",
    "    \"\"\"\n",
    "    Creates an ONNX Runtime session and runs inference on a given ONNX model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The file path to the ONNX model.\n",
    "        input_data (np.ndarray): The input data for the model,\n",
    "                                 formatted as a NumPy array.\n",
    "                                 Ensure its shape and dtype match the model's expected input.\n",
    "\n",
    "    Returns:\n",
    "        list: The output(s) from the ONNX model inference.\n",
    "              Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an inference session\n",
    "        # You can specify providers, e.g., providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        # to prioritize GPU if available.\n",
    "        session = rt.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "        print(f\"ONNX session created successfully for model: {model_path}\")\n",
    "\n",
    "        # Get input and output names\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        output_name = session.get_outputs()[0].name\n",
    "        print(f\"Model input name: {input_name}\")\n",
    "        print(f\"Model output name: {output_name}\")\n",
    "\n",
    "        # Run inference\n",
    "        # The run method takes a list of output names (or None for all outputs)\n",
    "        # and a dictionary of input names to input data.\n",
    "        outputs = session.run([output_name], {input_name: input_data})\n",
    "        print(\"Inference completed successfully.\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Example Usage ---\n",
    "    # 1. Create a dummy ONNX model for demonstration purposes.\n",
    "    #    In a real scenario, you would load an existing .onnx file.\n",
    "    #    This part requires onnx and numpy to be installed.\n",
    "\n",
    "    # This is a very simple dummy model: input -> add 1.0 -> output\n",
    "    try:\n",
    "        import onnx\n",
    "        from onnx import helper\n",
    "        from onnx import TensorProto\n",
    "\n",
    "        # Define the graph (nodes, inputs, outputs)\n",
    "        # Input: 'input_tensor' (float32, shape [1, 3, 224, 224])\n",
    "        input_tensor = helper.make_tensor_value_info('input_tensor', TensorProto.FLOAT, [1, 3, 224, 224])\n",
    "        # Output: 'output_tensor' (float32, same shape as input)\n",
    "        output_tensor = helper.make_tensor_value_info('output_tensor', TensorProto.FLOAT, [1, 3, 224, 224])\n",
    "\n",
    "        # Node: Add constant 1.0 to input_tensor\n",
    "        one_const = helper.make_tensor('one_const', TensorProto.FLOAT, [], [1.0]) # Scalar 1.0\n",
    "        node_def = helper.make_node(\n",
    "            'Add',\n",
    "            ['input_tensor', 'one_const'], # Inputs to the Add operation\n",
    "            ['output_tensor'],             # Output of the Add operation\n",
    "            name='add_one_node'\n",
    "        )\n",
    "\n",
    "        # Graph: Define the model graph\n",
    "        graph_def = helper.make_graph(\n",
    "            [node_def],\n",
    "            'simple_add_model',\n",
    "            [input_tensor],\n",
    "            [output_tensor],\n",
    "            [one_const] # Initializers (constants) used in the graph\n",
    "        )\n",
    "\n",
    "        # Model: Define the ONNX model\n",
    "        model_def = helper.make_model(graph_def, producer_name='onnx-example')\n",
    "        model_def.opset_import[0].version = 13 # Set opset version\n",
    "\n",
    "        onnx_model_path = \"dummy_model.onnx\"\n",
    "        onnx.save(model_def, onnx_model_path)\n",
    "        print(f\"\\nDummy ONNX model saved to {onnx_model_path}\")\n",
    "\n",
    "        # 2. Prepare dummy input data\n",
    "        dummy_input = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "        print(f\"Dummy input data shape: {dummy_input.shape}, dtype: {dummy_input.dtype}\")\n",
    "\n",
    "        # 3. Run the function with the dummy model and input\n",
    "        print(\"\\n--- Running ONNX session ---\")\n",
    "        model_outputs = create_and_run_onnx_session(onnx_model_path, dummy_input)\n",
    "\n",
    "        if model_outputs is not None:\n",
    "            print(\"\\nModel output(s):\")\n",
    "            for i, output in enumerate(model_outputs):\n",
    "                print(f\"Output {i} shape: {output.shape}, dtype: {output.dtype}\")\n",
    "                # print(f\"Output {i} (first 5 elements): {output.flatten()[:5]}\")\n",
    "                # You can compare with expected output: dummy_input + 1.0\n",
    "                # print(f\"Expected output (first 5 elements): {(dummy_input + 1.0).flatten()[:5]}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\n'onnx' library not found. Skipping dummy model creation.\")\n",
    "        print(\"Please install it (`pip install onnx`) if you want to run the dummy example.\")\n",
    "        print(\"You can still use the `create_and_run_onnx_session` function with your own .onnx file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during dummy model creation or execution: {e}\")\n",
    "\n",
    "    # To run with your own model:\n",
    "    # my_model_path = \"path/to/your/model.onnx\"\n",
    "    # my_input_data = np.random.rand(1, 10).astype(np.float32) # Adjust shape and dtype for your model\n",
    "    # my_outputs = create_and_run_onnx_session(my_model_path, my_input_data)\n",
    "    # if my_outputs:\n",
    "    #     print(\"Your model outputs:\", my_outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
